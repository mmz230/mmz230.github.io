{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbA3BE7HImlw"
      },
      "source": [
        "# Machine Learning Assignment\n",
        "\n",
        "Welcome to the Machine Learning assignment! This assignment is designed to test your understanding of the fundamental concepts of machine learning. You will be working with a dataset to implement and evaluate various machine learning models. Please follow the instructions carefully and ensure you understand each step before proceeding. Good luck!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE8hRRAJJj2I"
      },
      "source": [
        "# 0. Dataset for Classification Task\n",
        "\n",
        "In this assignment, you will be working with a dataset designed for a binary classification task. The dataset consists of three features and a target variable. Here's a brief overview:\n",
        "\n",
        "- **Features**: The dataset contains three features (`Feature_1`, `Feature_2`, and `Feature_3`). These features are numerical and may have varying scales.\n",
        "\n",
        "- **Target**: The target variable is binary, indicating two classes. The distribution of classes might vary, so it's crucial to check the class balance before proceeding with model training.\n",
        "\n",
        "To generate your dataset, follow the instructions in section 0.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBXkkvg4IOO-"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def generate_dataset(student_id):\n",
        "    # Seed the random number generator\n",
        "    np.random.seed(student_id)\n",
        "\n",
        "    # Randomly decide class separation\n",
        "    class_sep_choices = [2.5, 1.0, 0.25]\n",
        "    class_sep = np.random.choice(class_sep_choices)\n",
        "\n",
        "    # Randomly decide class balance\n",
        "    weights_choices = [[0.8, 0.2],[0.6, 0.4], [0.7, 0.3], [0.5, 0.5], [0.4, 0.6], [0.3, 0.7],[0.2, 0.8]]\n",
        "    weights = weights_choices[np.random.randint(len(weights_choices))]\n",
        "\n",
        "    # Generate dataset using make_classification\n",
        "    X, y = make_classification(n_samples=1000, n_features=3, n_informative=3, n_redundant=0, n_classes=2, class_sep=class_sep, weights=weights, random_state=student_id)\n",
        "\n",
        "    # Convert to DataFrame for better visualization\n",
        "    df = pd.DataFrame(X, columns=['Feature_1', 'Feature_2', 'Feature_3'])\n",
        "    df['Target'] = y\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p4owx2PUD6d"
      },
      "source": [
        "## 0.1. Generating Your Dataset\n",
        "\n",
        "To generate your dataset for this assignment, you need to:\n",
        "\n",
        "1. Run the cell above to load the dataset generation function.\n",
        "2. In the cell below, replace `YOUR_STUDENT_ID_1` and `YOUR_STUDENT_ID_2` with your actual student IDs and then run the cell. This will generate your dataset for the assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ3nUjo-TGHz"
      },
      "outputs": [],
      "source": [
        "# Replace YOUR_STUDENT_ID with your actual student ID\n",
        "student_id_1 = YOUR_STUDENT_ID_1 # Student Id for the first student in the group. Example = 2022...\n",
        "student_id_2 = YOUR_STUDENT_ID_2 # Student Id for the second student in the group.  Example = 2022...\n",
        "# Note: If only one person in the group then enter your ID(same id) in both the above fields\n",
        "dataset = generate_dataset(student_id_1 + student_id_2)\n",
        "dataset.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eyimV42UPdR"
      },
      "source": [
        "# 1.1 Data Exploration\n",
        "\n",
        "Data exploration is the initial step in data analysis, where users explore a large dataset in an unstructured manner to uncover initial properties, characteristics, and patterns in the data. It's crucial because it allows you to understand the dataset's underlying structure, detect outliers or anomalies, and find interesting patterns.\n",
        "\n",
        "For this assignment, we will guide you through some basic techniques of data exploration. While we provide specific methods to explore, you are encouraged to dive deeper and explore additional techniques as you see fit.\n",
        "\n",
        "Let's start with some basic statistical measures: Mean, Median, and Mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZjHYbFSUNxE"
      },
      "outputs": [],
      "source": [
        "def compute_statistics(data):\n",
        "    \"\"\"\n",
        "    Compute basic statistics: Mean, Median, and Mode for the given data.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - Each value should be rounded to 3 decimal places\n",
        "    - mean: A dictionary with the mean of each feature.\n",
        "    - median: A dictionary with the median of each feature.\n",
        "    - mode: A dictionary with the mode of each feature.\n",
        "    \"\"\"\n",
        "    # TODO: Replace the 'None' with your code to calculate the mean\n",
        "    mean = None\n",
        "\n",
        "    # TODO: Replace the 'None' with your code to calculate the median\n",
        "    median = None\n",
        "\n",
        "    # TODO: Replace the 'None' with your code to calculate the mode\n",
        "    mode = None\n",
        "    # Round each one to 3 decimal places\n",
        "    statistics = {\n",
        "        'mean': mean,\n",
        "        'median': median,\n",
        "        'mode': mode\n",
        "    }\n",
        "    return statistics\n",
        "compute_statistics(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p4mepuKv7LP"
      },
      "source": [
        "# 1.2 Dataset Distribution Exploration\n",
        "\n",
        "Understanding the distribution of your dataset, particularly the target variable, is a fundamental aspect of preparing your data for machine learning. The distribution tells you about the balance or imbalance between classes, which is essential since imbalanced classes can significantly affect the performance and evaluation of your machine learning models.\n",
        "\n",
        "For classification tasks, it's crucial to know whether one class is overrepresented compared to others. If one class dominates, the model might simply learn to always predict that class, ignoring the features.\n",
        "\n",
        "In this section, you will learn how to visualize the distribution of the target variable in your dataset using a pie chart. This type of visualization will provide you with a clear and immediate understanding of the class balance.\n",
        "\n",
        "\n",
        "Below is a function `plot_target_distribution` that generates a pie chart that visualizes the distribution of the target variable in your dataset.\n",
        "\n",
        "This exercise is for your learning and understanding of the data you will be working with. There are no graded questions associated with this section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7R2RCflv8MG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_target_distribution(data):\n",
        "    \"\"\"\n",
        "    Plot the distribution of the target variable using a pie chart.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "\n",
        "    Implements:\n",
        "    - A pie chart visualizing the distribution of the target variable.\n",
        "    \"\"\"\n",
        "    # Calculate the distribution of the target variable\n",
        "    target_counts = data['Target'].value_counts(normalize=True).round(2)\n",
        "\n",
        "    # Create a pie chart\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.pie(target_counts, labels=target_counts.index, autopct='%1.0f%%', startangle=140)\n",
        "    plt.title('Distribution of the Target Variable')\n",
        "    plt.show()\n",
        "\n",
        "plot_target_distribution(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4opV8O5y7R7"
      },
      "source": [
        "# 1.3 Visualizing Dataset Properties with Scatter Plots\n",
        "\n",
        "Visualizing your dataset can provide valuable insights into its structure and properties. A scatter plot is a useful visual tool for understanding the relationships between variables in your dataset. For datasets with three features, a 3D scatter plot can be particularly insightful. It allows you to observe the interaction between the features and how they contribute to the separability of the target classes.\n",
        "\n",
        "In this section, you will create a 3D scatter plot to visualize the three features of your dataset. By examining this plot, you can gain insights into the dataset's structure, such as the degree of overlap between classes, which can inform your choice of classification models and preprocessing steps.\n",
        "\n",
        "**Hint**: You can use `Axes3D` from `mpl_toolkits.mplot3d` to create a 3D scatter plot. You can import it using `from mpl_toolkits.mplot3d import Axes3D`.\n",
        "\n",
        "A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0n_5JAuwMvi"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace the 'None' with your code to create a 3D scatter plot\n",
        "def plot_3d_scatter(data):\n",
        "    \"\"\"\n",
        "    Plot a 3D scatter plot of the dataset to visualize the three features.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - A 3D scatter plot visualizing the dataset's features and target variable.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "plot_3d_scatter(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZVUAGxV0_NP"
      },
      "source": [
        "## 1.4 Visualizing Data Distribution with Box Plots\n",
        "\n",
        "Box plots are a convenient way of visually displaying the data distribution through their quartiles. They can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n",
        "\n",
        "Here's what each part of a box plot represents:\n",
        "\n",
        "- **Median (Q2/50th Percentile)**: the middle value of the dataset.\n",
        "- **First quartile (Q1/25th Percentile)**: the median of the first half of the dataset.\n",
        "- **Third quartile (Q3/75th Percentile)**: the median of the second half of the dataset.\n",
        "- **Interquartile range (IQR)**: the distance between the first and third quartiles.\n",
        "- **Whiskers**: the lines that extend from the quartiles to the maximum and minimum values, excluding outliers.\n",
        "- **Outliers**: points that fall below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\n",
        "\n",
        "By visualizing these aspects, box plots allow us to compare the distributions of different features or datasets in a compact manner.\n",
        "\n",
        "Let's create box plots for each feature in your dataset to analyze their distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R4_cjdDzHmn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_boxplots(data):\n",
        "    \"\"\"\n",
        "    Plot box plots for each feature in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - Box plots visualizing the distribution of each feature.\n",
        "    \"\"\"\n",
        "    # Prepare the figure\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # Create a box plot for each feature in the dataset\n",
        "    data.boxplot(column=['Feature_1', 'Feature_2', 'Feature_3'])\n",
        "\n",
        "    # Set the title and labels\n",
        "    plt.title('Box plot for each feature')\n",
        "    plt.ylabel('Value')\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot the box plots\n",
        "plot_boxplots(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVWBeVf63PrM"
      },
      "source": [
        "## 1.5 Visualizing Feature Distribution with Density Plots\n",
        "\n",
        "When preparing data for machine learning, it's important to understand the scale of each feature. Features on different scales can influence the model in disproportionate ways, especially in algorithms that are sensitive to the magnitude of the data, like SVMs or k-NN.\n",
        "\n",
        "Density plots are useful for visualizing the distribution of each feature on its own scale. By examining these plots, we can determine if features need to be rescaled or normalized so that no single feature dominates the learning algorithm due to its scale.\n",
        "\n",
        "Let's create density plots for each feature in your dataset to analyze their distribution and compare their scales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TOkOgtQ1GIA"
      },
      "outputs": [],
      "source": [
        "def plot_density_scaling(data):\n",
        "    \"\"\"\n",
        "    Plot density plots for all features on the same graph to visualize their distribution before scaling.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - A combined density plot visualizing the distribution of all features.\n",
        "    \"\"\"\n",
        "    features = data.drop(columns=['Target'])\n",
        "\n",
        "    # Plotting combined density plots for all features\n",
        "    features.plot(kind='density', figsize=(10, 6))\n",
        "    plt.title('Density Plots of Features Before Scaling')\n",
        "    plt.xlabel('Attribute Value')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend(title='Features')\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.show()\n",
        "\n",
        "plot_density_scaling(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIz3rttR5D3P"
      },
      "source": [
        "# 2 k-NN Classification\n",
        "\n",
        "## 2.1 k-NN Classification Using Raw/Unscaled Data\n",
        "\n",
        "In this section, you will apply the k-Nearest Neighbors (k-NN) algorithm to classify data points using the raw values of the features. You will use a fixed `k=5` for the number of neighbors to maintain consistency in the classification process.\n",
        "\n",
        "You will perform a train-test split and report the model's performance metrics based on the testing set(to answer the questions as well). This will help you understand how well the model is learning from the data provided.\n",
        "\n",
        "After training the classifier, you will compute a confusion matrix on the testing set, which will give you detailed insights into the true positives, true negatives, false positives, and false negatives(and answer the question based on that). These metrics are essential for a comprehensive understanding of your model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PoAAtG83Wmc"
      },
      "outputs": [],
      "source": [
        "# Hint you can use the following functions from sklearn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def knn_raw_data_metrics(data):\n",
        "    \"\"\"\n",
        "    Perform k-NN classification on raw data and compute the confusion matrix based on the testing set.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary containing the confusion matrix and other relevant metrics based on the testing set.\n",
        "    \"\"\"\n",
        "    # Split the data into features and target\n",
        "    # TODO: Replace None with the appropriate code to extract features and target\n",
        "    X = None\n",
        "    y = None\n",
        "\n",
        "    # TODO: Replace None with the appropriate code to split the data\n",
        "    # Use a fixed random_state to ensure reproducibility\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize the k-NN classifier with k=5 =>n_neighbors=5\n",
        "    knn = None\n",
        "\n",
        "    # TODO: Replace None with the appropriate code to fit the classifier and make predictions on the training set\n",
        "\n",
        "    # Compute the confusion matrix based on the testing set\n",
        "    cm_test = None\n",
        "    # TODO: Replace None with the appropriate code to calculate other metrics based on the testing set\n",
        "    # Note: Convert the metrics to percentage(out of 100) and round them to the nearest integer\n",
        "    metrics_test = {\n",
        "        'accuracy': None,\n",
        "        'precision': None,\n",
        "        'recall': None,\n",
        "    }\n",
        "\n",
        "    return metrics_test, cm_test\n",
        "knn_raw_data_metrics(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk0aYPsgBCDp"
      },
      "source": [
        "## 2.2 Finding the Best Value for \\( k \\) using Cross-Validation\n",
        "\n",
        "Cross-validation is a technique used to assess how the results of a statistical analysis will generalize to an independent dataset. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
        "\n",
        "For k-NN, selecting the right \\( k \\) is crucial as it determines how well the model generalizes to new data. A \\( k \\) that's too low can lead to overfitting, where the model captures noise in the data. Conversely, a \\( k \\) that's too high can lead to underfitting, where the model can't capture the underlying trends.\n",
        "\n",
        "\n",
        "You will perform 10-Fold cross-validation for values of \\( k \\) ranging from 1 to 30 and plot the error rates to find the best \\( k \\). Remember to set a fixed `random_state` in your K-Fold to ensure that your results are reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE2So7x9-1zW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "def find_best_k_using_cross_validation(data):\n",
        "    # TODO: Split the data into features (X) and target (y)\n",
        "    # X = ...\n",
        "    # y = ...\n",
        "\n",
        "    # TODO: Initialize a list to store error rates for each k value\n",
        "    error_rates = []\n",
        "\n",
        "    # Define the K-Fold cross-validator with a fixed random state -- dont change this\n",
        "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "    # TODO: Loop over k values from 1 to 30\n",
        "    for k in range(1, 31):\n",
        "        # TODO: Implement k-NN with current k value\n",
        "        # knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "        # TODO: Use cross_val_score to perform cross-validation\n",
        "        # scores = cross_val_score(knn, X, y, cv=kf)\n",
        "\n",
        "        # TODO: Compute the mean error rate and append to the list\n",
        "        # error_rate = ...\n",
        "        # error_rates.append(error_rate)\n",
        "\n",
        "    # TODO: Find the k value with the minimum error rate\n",
        "    # best_k = ...\n",
        "\n",
        "    # TODO: Return the best k value and the error rates list\n",
        "    # Note: Remember to round the best(min)_error_rate to 2 decimal places\n",
        "    return best_k, error_rates,best_error_rate\n",
        "\n",
        "best_k, error_rates,best_error_rate = find_best_k_using_cross_validation(dataset)\n",
        "\n",
        "# Plotting the error rates\n",
        "print(f\"The best k value is: {best_k} and {best_error_rate}\")\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 31), error_rates, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
        "plt.title('Error Rate vs. K Value')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McP2--PhdBSj"
      },
      "source": [
        "## 2.3 Effects of Scaling on Data\n",
        "\n",
        "Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. Scaling is critical for algorithms that calculate distances between data points, such as k-Nearest Neighbors (k-NN), because features on larger scales can unduly influence the algorithm.\n",
        "\n",
        "In this section, we will scale our dataset using `StandardScaler` from `sklearn.preprocessing`. This scaler removes the mean and scales each feature/variable to unit variance. This standardization of features ensures that each feature contributes equally to the distance computations.\n",
        "\n",
        "We will then compare the range of values (max - min) for each feature before and after scaling to understand the effect of this transformation. Additionally, we will visualize the density vs. value graphs for each feature to see how scaling affects the distribution of the data.\n",
        "\n",
        "You need to proceed with scaling the data, plotting the density graphs, and computing the range of values for each feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMciyl3zdBuX"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def scale_data_plot_and_compute_ranges(data):\n",
        "    \"\"\"\n",
        "    Scale the data using StandardScaler, plot the density vs. value graphs for each feature,\n",
        "    and compute the range of values for each feature before and after scaling.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - ranges_before: A dictionary with the range of values for each feature before scaling.\n",
        "    - ranges_after: A dictionary with the range of values for each feature after scaling.\n",
        "    \"\"\"\n",
        "    # Separate the features from the target variable\n",
        "    features = data.drop(columns=['Target'])\n",
        "\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler to the features and transform them (do not return anything yet)\n",
        "    # scaled_features = ...\n",
        "\n",
        "    # Create a DataFrame from the scaled features with the same column names as the original features DataFrame\n",
        "    # scaled_df = ...\n",
        "\n",
        "    # Compute the range (max - min) for each feature before scaling\n",
        "    # ranges_before = ...\n",
        "\n",
        "    # Compute the range (max - min) for each feature after scaling\n",
        "    # ranges_after = ...\n",
        "\n",
        "    # Plot the density plots for the original features\n",
        "    # features.plot(kind='density', title='Density Plots of Features Before Scaling')\n",
        "    # plt.show()\n",
        "\n",
        "    # Plot the density plots for the scaled features\n",
        "    # scaled_df.plot(kind='density', title='Density Plots of Features After Scaling')\n",
        "    # plt.show()\n",
        "\n",
        "    # Return the computed ranges\n",
        "    # Note: You need to round the ranges to nearest integer\n",
        "    return ranges_before, ranges_after\n",
        "\n",
        "# Call the function with the dataset\n",
        "ranges_before, ranges_after = scale_data_plot_and_compute_ranges(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoCz4yXGlWaf"
      },
      "source": [
        "## 2.4 Finding the Optimal k for k-NN using Normalized Data\n",
        "\n",
        "In this section, you'll explore how normalization affects the performance of the k-Nearest Neighbors (k-NN) classifier. We will use 10-fold-cross-validation to determine the best `k` value for our classifier.\n",
        "\n",
        "Your task is to implement the cross-validation process for `k` values ranging from 1 to 30 and to identify the `k` value that minimizes the error rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUiFlblWdr52"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def find_best_k_using_cross_validation_normalized(data):\n",
        "    \"\"\"\n",
        "    Normalize the data using StandardScaler and evaluate k-NN performance with normalized data.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - normalized_data: A DataFrame containing the normalized data.\n",
        "    - error_rates: A list containing error rates for each k value with normalized data.\n",
        "    \"\"\"\n",
        "    # TODO: Normalize the data using StandardScaler\n",
        "    # scaler = StandardScaler()\n",
        "    # X_scaled = ...\n",
        "\n",
        "    # TODO: Split the normalized data into features (X) and target (y)\n",
        "    # X = ...\n",
        "    # y = ...\n",
        "\n",
        "\n",
        "    # TODO: Initialize a list to store error rates for each k value\n",
        "    error_rates = []\n",
        "\n",
        "    # Define the K-Fold cross-validator with a fixed random state -- do not change this\n",
        "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "    # TODO: Loop over k values from 1 to 30\n",
        "    for k in range(1, 31):\n",
        "        # TODO: Implement k-NN with the current k value\n",
        "        # knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "        # TODO: Use cross_val_score to perform cross-validation\n",
        "        # scores = cross_val_score(knn, X, y, cv=kf)\n",
        "\n",
        "        # TODO: Compute the mean error rate and append to the list\n",
        "        # error_rate = ...\n",
        "        # error_rates.append(error_rate)\n",
        "\n",
        "    # TODO: Find the k value with the minimum error rate\n",
        "    best_k = ...\n",
        "    best_error_rate = ...\n",
        "\n",
        "    # TODO: Return the best k value and the error rates list\n",
        "    # Note: Please round the best(min) error rate to 2 decimal point\n",
        "    return best_k, error_rates,best_error_rate\n",
        "\n",
        "best_k, error_rates,best_error_rate = find_best_k_using_cross_validation_normalized(dataset)\n",
        "\n",
        "# Plotting the error rates\n",
        "print(f\"The best k value is: {best_k} and {best_error_rate}\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 31), error_rates, color='blue', linestyle='dashed', marker='o',\n",
        "         markerfacecolor='red', markersize=10)\n",
        "plt.title('Error Rate vs. K Value')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb87rp26qBLh"
      },
      "source": [
        "## 2.5 k-NN Classification on Normalized Data\n",
        "\n",
        "In this section, we will address the importance of data preprocessing and its impact on the k-Nearest Neighbors (k-NN) algorithm. Proper preprocessing is crucial to prevent data leakage and ensure that our model generalizes well to unseen data.\n",
        "\n",
        "### Process Overview\n",
        "\n",
        "1. **Split the Data**: Start by dividing your dataset into a training set and a testing set. This step is essential to prevent any information from the test set from \"leaking\" into the training process.\n",
        "\n",
        "2. **Normalize the Data**: After splitting, use a normalization technique such as `StandardScaler` to fit only on the training data. Then, apply this scaler to transform both the training and testing sets. This ensures that the test data is a fair representation of new, unseen data and is not influenced by the training set.\n",
        "\n",
        "3. **Train the k-NN Classifier**: Train your k-NN classifier on the normalized training data. Use the optimal `k` value that you determined from previous exercises to achieve the best classification performance.\n",
        "\n",
        "4. **Evaluate the Model**: Finally, evaluate your model's performance using the normalized test set. Compute the accuracy, precision and recall. Additionally, generate a confusion matrix to gain detailed insights into the true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "By following this revised process, you will ensure that your evaluation of the k-NN classifier is based on a robust and unbiased assessment of its predictive performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvzUIXW8qBjb"
      },
      "outputs": [],
      "source": [
        "# Hint you can use the following functions from sklearn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def knn_normalized_data_metrics(data, k):\n",
        "    \"\"\"\n",
        "    Perform k-NN classification on data with normalization applied after splitting into training and testing sets.\n",
        "    Compute the confusion matrix and other relevant metrics based on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset.\n",
        "    - k: The optimal k value for the k-NN classifier.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary containing the confusion matrix and other relevant metrics based on the test set.\n",
        "    \"\"\"\n",
        "    # Split the data into training and testing sets first\n",
        "    X = ..\n",
        "    y = ...\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Do not change the random state\n",
        "\n",
        "    # Normalize the training data and then apply the same transformation to the test data\n",
        "    # Remember to prevent data leakage, only train/fit on training data, for testing data you just need to transform\n",
        "\n",
        "    # Initialize the k-NN classifier with the optimal k\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    # Fit the classifier on the normalized training data and make predictions on the normalized test set\n",
        "    y_pred = ...\n",
        "\n",
        "    # Compute the confusion matrix and other metrics based on the test set and round to nearest integer\n",
        "    cm_test = ...\n",
        "    # Note: Convert the metrics to percentage(out of 100) and round them to the nearest integer\n",
        "    metrics_test = {\n",
        "        'accuracy': ...,\n",
        "        'precision': ...,\n",
        "        'recall': ...\n",
        "    }\n",
        "\n",
        "    return metrics_test, cm_test\n",
        "\n",
        "# Replace 'optimal_k' with the actual optimal k value you have determined\n",
        "optimal_k = # ... (the optimal k value)\n",
        "\n",
        "# Call the function with the dataset and the optimal k value\n",
        "knn_normalized_data_metrics(dataset, optimal_k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqhERiF8_daw"
      },
      "source": [
        "# 3 Perceptron Classifier on Normalized Data\n",
        "\n",
        "The Perceptron is a fundamental linear classifier in machine learning, particularly effective for binary classification problems. In this section, you will learn how to properly train a Perceptron classifier on a dataset where normalization is applied correctly to prevent data leakage.\n",
        "\n",
        "### Step 1: Splitting the Data\n",
        "Before any data preprocessing, split your dataset into training and testing sets. This is a crucial step to prevent the leakage of information from the test set into the training process.\n",
        "\n",
        "### Step 2: Normalizing the Training Data\n",
        "Normalize `Feature_1` and `Feature_2` in the training data using a standard scaler. It's important to fit the scaler only on the training data and then transform both the training and testing sets with it.\n",
        "\n",
        "### Step 3: Visualizing the Training Data\n",
        "Visualize the distribution of the normalized `Feature_1` and `Feature_2` in the training data using a scatter plot. Understanding the structure of the data is essential before applying any machine learning model.\n",
        "\n",
        "### Step 4: Training the Perceptron\n",
        "With the normalized training data, train the Perceptron classifier. Your goal is to find an optimal decision boundary that can effectively distinguish between the two classes.\n",
        "\n",
        "### Step 5: Visualizing the Decision Boundary on Training Data\n",
        "After training, visualize the decision boundary on a scatter plot of the normalized training data. This will help you understand how well the Perceptron has learned to classify the training data.\n",
        "\n",
        "### Step 6: Visualizing the Testing Data with the Decision Boundary\n",
        "Apply the trained Perceptron to the normalized testing data and visualize the decision boundary. This will show how well the classifier generalizes to new, unseen data.\n",
        "\n",
        "### Step 7: Evaluating Classifier Performance\n",
        "Finally, evaluate the Perceptron's performance on the normalized testing set using metrics such as accuracy, recall, and precision. These metrics will give you a comprehensive view of the classifier's effectiveness on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAwxXUy3_eES"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def perceptron_classifier_on_normalized_data(data):\n",
        "    \"\"\"\n",
        "    Train a Perceptron classifier on data with normalization applied after splitting into training and testing sets.\n",
        "    Evaluate its performance and visualize the decision boundary.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the dataset with features and target.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary containing the classifier's performance metrics.\n",
        "    - Slope and intercept of the decision boundary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split the data into training and testing sets first\n",
        "    # TODO: Extract Feature_1 and Feature_2, then split the data\n",
        "    features = ...\n",
        "    target = ...\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Do not change the random state\n",
        "\n",
        "    # Normalize the training data and then apply the same transformation to the test data\n",
        "    # Remember to prevent data leakage, only train/fit on training data, for testing data you just need to transform\n",
        "\n",
        "    # Visualize the normalized training data\n",
        "    # TODO: Plot a scatter plot of the normalized training data\n",
        "\n",
        "    # Train the Perceptron classifier on the normalized training data\n",
        "    # Initialize the Perceptron and fit it to the normalized training data\n",
        "    perceptron = Perceptron(max_iter=1000, eta0=0.1, random_state=42)\n",
        "\n",
        "    # Evaluate the classifier's performance on the normalized testing set\n",
        "    # TODO: Make predictions on the normalized testing set and calculate performance metrics\n",
        "    # Note: Convert the metrics to percentage(out of 100) and round them to the nearest integer\n",
        "    metrics = {\n",
        "        'accuracy': ...,\n",
        "        'precision': ...,\n",
        "        'recall': ...,\n",
        "    }\n",
        "\n",
        "    # Calculate the slope and intercept of the decision boundary (round to 2 decimal)\n",
        "    # TODO: Calculate the coefficients and intercept from the Perceptron to determine the decision boundary\n",
        "    slope = ...\n",
        "    intercept = ...\n",
        "\n",
        "    # Visualize the decision boundary on the normalized training data\n",
        "    # TODO: Plot the decision boundary on a scatter plot of the normalized training data\n",
        "\n",
        "    # Visualize the decision boundary on the normalized testing data\n",
        "    # TODO: Plot the decision boundary on a scatter plot of the normalized testing data\n",
        "\n",
        "    return metrics, slope, intercept\n",
        "\n",
        "perceptron_classifier_on_normalized_data(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT5cUwDS0ZDh"
      },
      "source": [
        "# 4 SVM Classifier\n",
        "\n",
        "Support Vector Machines (SVM) are a set of supervised learning methods used for classification, regression, and outliers detection. The advantages of support vector machines are:\n",
        "\n",
        "- Effective in high dimensional spaces.\n",
        "- Still effective in cases where the number of dimensions is greater than the number of samples.\n",
        "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "- Versatile: different kernel functions can be specified for the decision function.\n",
        "\n",
        "For this exercise, you will:\n",
        "\n",
        "1. Split your data into training and testing sets to prevent data leakage during normalization.\n",
        "2. Normalize your features within the training set since SVMs are sensitive to the scaling of the data. Then apply the same transformation to the test set.\n",
        "3. Train an SVM classifier using a \\\"linear\\\" kernel to find the optimal separating hyperplane.\n",
        "4. Evaluate the classifier's performance with metrics such as accuracy, precision, and recall on test set.\n",
        "5. Display the confusion matrix for classifier on test set to understand the true versus predicted labels.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLgVy-lRC7SL"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "def train_svm_and_evaluate(data):\n",
        "    \"\"\"\n",
        "    Train an SVM classifier, compute performance metrics, and print coefficients and intercept.\n",
        "\n",
        "    Parameters:\n",
        "    - data: A pandas DataFrame containing the normalized dataset.\n",
        "\n",
        "    Returns:\n",
        "    - Coefficients and intercept of the trained SVM model\n",
        "    - Accuracy, Recall, Precision\n",
        "    \"\"\"\n",
        "    # TODO: Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Dont change the random state\n",
        "    # TODO: Normalize the training data and apply the same transformation to the test data\n",
        "    # TODO: Train an SVM classifier using the kernel='linear' option\n",
        "    svm_clf = SVC(kernel='linear', random_state=42) #Dont change the given options\n",
        "    # TODO: Evaluate the performance on the test set and report the metrics\n",
        "    # TODO: Compute and display the confusion matrix\n",
        "    # Note: Convert the metrics to percentage(out of 100) and round them to the nearest integer\n",
        "    return {\n",
        "        'intercept': intercept,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# Call the function\n",
        "train_svm_and_evaluate(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY6NtiQr85z2"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Throughout this assignment, we have explored various machine learning classifiers, delved into the importance of data normalization, and understood the significance of evaluating model performance using different metrics. We have also emphasized the necessity of proper data handling to prevent data leakage and ensure the robustness of our models.\n",
        "\n",
        "As we conclude this assignment, reflect on the insights gained and the skills developed. These will serve as a solid foundation for tackling more complex machine learning challenges in the future.\n",
        "\n",
        "**Thank you for your diligent work, and best wishes for your continued learning journey!**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
